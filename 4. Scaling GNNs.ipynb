{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhZdpF_LY1O7","executionInfo":{"status":"ok","timestamp":1667225622095,"user_tz":240,"elapsed":22513,"user":{"displayName":"Anh Dao","userId":"04516934983542627501"}},"outputId":"a49c1408-f7bb-4fe3-f6ce-c632d0a260ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n","\u001b[K     |████████████████████████████████| 7.9 MB 14.4 MB/s \n","\u001b[K     |████████████████████████████████| 3.5 MB 8.6 MB/s \n","\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install required packages.\n","import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"]},{"cell_type":"code","source":["import torch\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n","\n","print()\n","print(f'Dataset: {dataset}:')\n","print('==================')\n","print(f'Number of graphs: {len(dataset)}')\n","print(f'Number of features: {dataset.num_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","\n","data = dataset[0]  # Get the first graph object.\n","\n","print()\n","print(data)\n","print('===============================================================================================================')\n","\n","# Gather some statistics about the graph.\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Number of training nodes: {data.train_mask.sum()}')\n","print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n","print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n","print(f'Has self-loops: {data.has_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k55Cnzh4Y6LK","executionInfo":{"status":"ok","timestamp":1667225749731,"user_tz":240,"elapsed":62564,"user":{"displayName":"Anh Dao","userId":"04516934983542627501"}},"outputId":"d16ee466-6c66-4be7-8a25-b97c6cd408e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n","Processing...\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dataset: PubMed():\n","==================\n","Number of graphs: 1\n","Number of features: 500\n","Number of classes: 3\n","\n","Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n","===============================================================================================================\n","Number of nodes: 19717\n","Number of edges: 88648\n","Average node degree: 4.50\n","Number of training nodes: 60\n","Training node label rate: 0.003\n","Has isolated nodes: False\n","Has self-loops: False\n","Is undirected: True\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n"]}]},{"cell_type":"markdown","metadata":{"id":"DEJtbHGs9JQ9"},"source":["As can be seen, this graph has around 19,717 nodes.\n","While this number of nodes should fit into GPU memory with ease, it's nonetheless a good example to showcase how one can scale GNNs up within PyTorch Geometric.\n","\n","**Cluster-GCN** ([Chiang et al. (2019)](https://arxiv.org/abs/1905.07953) works by first partioning the graph into subgraphs based on graph partitioning algorithms.\n","With this, GNNs are restricted to solely convolve inside their specific subgraphs, which omits the problem of **neighborhood explosion**."]},{"cell_type":"markdown","metadata":{"id":"sp__L20M-qns"},"source":["However, after the graph is partitioned, some links are removed which may limit the model's performance due to a biased estimation.\n","To address this issue, Cluster-GCN also **incorporates between-cluster links inside a mini-batch**, which results in the following **stochastic partitioning scheme**:"]},{"cell_type":"markdown","metadata":{"id":"RwuwXWLZ_4OQ"},"source":["Here, colors represent the adjacency information that is maintained per batch (which is potentially different for every epoch).\n","\n","PyTorch Geometric provides a **two-stage implementation** of the Cluster-GCN algorithm:\n","1. [**`ClusterData`**](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.ClusterData) converts a `Data` object into a dataset of subgraphs containing `num_parts` partitions.\n","2. Given a user-defined `batch_size`, [**`ClusterLoader`**](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.ClusterLoader) implements the stochastic partitioning scheme in order to create mini-batches.\n","\n","The procedure to craft mini-batches then looks as follows:"]},{"cell_type":"code","source":["from torch_geometric.loader import ClusterData, ClusterLoader\n","\n","torch.manual_seed(12345)\n","cluster_data = ClusterData(data, num_parts = 128) # 1. Create subgraphs\n","train_loader = ClusterLoader(cluster_data, batch_size = 32, shuffle = True)\n","\n","print()\n","total_num_nodes = 0\n","for step, sub_data in enumerate(train_loader):\n","  print(f'Step {step + 1}:')\n","  print('=======')\n","  print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n","  print(sub_data)\n","  print()\n","  total_num_nodes += sub_data.num_nodes\n","\n","print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bBZXJBUaLDm","executionInfo":{"status":"ok","timestamp":1667226113891,"user_tz":240,"elapsed":357,"user":{"displayName":"Anh Dao","userId":"04516934983542627501"}},"outputId":"90746a18-3aad-40ef-99d2-66cd5ad5d34f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Computing METIS partitioning...\n"]},{"output_type":"stream","name":"stdout","text":["\n","Step 1:\n","=======\n","Number of nodes in the current batch: 4928\n","Data(x=[4928, 500], y=[4928], train_mask=[4928], val_mask=[4928], test_mask=[4928], edge_index=[2, 16174])\n","\n","Step 2:\n","=======\n","Number of nodes in the current batch: 4937\n","Data(x=[4937, 500], y=[4937], train_mask=[4937], val_mask=[4937], test_mask=[4937], edge_index=[2, 17832])\n","\n","Step 3:\n","=======\n","Number of nodes in the current batch: 4927\n","Data(x=[4927, 500], y=[4927], train_mask=[4927], val_mask=[4927], test_mask=[4927], edge_index=[2, 14712])\n","\n","Step 4:\n","=======\n","Number of nodes in the current batch: 4925\n","Data(x=[4925, 500], y=[4925], train_mask=[4925], val_mask=[4925], test_mask=[4925], edge_index=[2, 18006])\n","\n","Iterated over 19717 of 19717 nodes!\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n"]}]},{"cell_type":"markdown","metadata":{"id":"rv1wKy6VBOp1"},"source":["Here, we partition the initial graph into **128 partitions**, and use a **`batch_size` of 32 subgraphs** to form mini-batches (leaving us with 4 batches per epoch).\n","As one can see, after a single epoch, each node has been seen exactly once.\n","\n","The great thing about Cluster-GCN is that it does not complicate the GNN model implementation.\n","Here, we can make use of the **exactly same architecture** introduced in [the second chapter of this tutorial](https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX)."]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels=16)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLFN3_Tma4-h","executionInfo":{"status":"ok","timestamp":1667226158650,"user_tz":240,"elapsed":239,"user":{"displayName":"Anh Dao","userId":"04516934983542627501"}},"outputId":"b9b21f46-5254-49b1-982b-783a56c33423"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GCN(\n","  (conv1): GCNConv(500, 16)\n","  (conv2): GCNConv(16, 3)\n",")\n"]}]},{"cell_type":"code","source":["from IPython.display import Javascript\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels=16)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","\n","      for sub_data in train_loader:  # Iterate over each mini-batch.\n","          out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n","          loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])  # Compute the loss solely based on the training nodes.\n","          loss.backward()  # Derive gradients.\n","          optimizer.step()  # Update parameters based on gradients.\n","          optimizer.zero_grad()  # Clear gradients.\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      \n","      accs = []\n","      for mask in [data.train_mask, data.val_mask, data.test_mask]:\n","          correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n","          accs.append(int(correct.sum()) / int(mask.sum()))  # Derive ratio of correct predictions.\n","      return accs\n","\n","for epoch in range(1, 51):\n","    loss = train()\n","    train_acc, val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"GxBsAdkPbEMb","executionInfo":{"status":"ok","timestamp":1667226173363,"user_tz":240,"elapsed":8642,"user":{"displayName":"Anh Dao","userId":"04516934983542627501"}},"outputId":"273cffbb-ea28-44ab-87a3-1ec33a92977b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Train: 0.3333, Val Acc: 0.4160, Test Acc: 0.4070\n","Epoch: 002, Train: 0.4833, Val Acc: 0.4420, Test Acc: 0.4270\n","Epoch: 003, Train: 0.7333, Val Acc: 0.5220, Test Acc: 0.5160\n","Epoch: 004, Train: 0.8333, Val Acc: 0.5920, Test Acc: 0.5930\n","Epoch: 005, Train: 0.9167, Val Acc: 0.6920, Test Acc: 0.6810\n","Epoch: 006, Train: 0.9000, Val Acc: 0.6680, Test Acc: 0.6680\n","Epoch: 007, Train: 0.8833, Val Acc: 0.6520, Test Acc: 0.6530\n","Epoch: 008, Train: 0.9167, Val Acc: 0.7020, Test Acc: 0.6940\n","Epoch: 009, Train: 0.9333, Val Acc: 0.7340, Test Acc: 0.7290\n","Epoch: 010, Train: 0.9333, Val Acc: 0.6880, Test Acc: 0.6900\n","Epoch: 011, Train: 0.9500, Val Acc: 0.7240, Test Acc: 0.7330\n","Epoch: 012, Train: 0.9500, Val Acc: 0.7500, Test Acc: 0.7300\n","Epoch: 013, Train: 0.9667, Val Acc: 0.7680, Test Acc: 0.7530\n","Epoch: 014, Train: 0.9667, Val Acc: 0.7600, Test Acc: 0.7450\n","Epoch: 015, Train: 0.9667, Val Acc: 0.7680, Test Acc: 0.7550\n","Epoch: 016, Train: 0.9667, Val Acc: 0.7660, Test Acc: 0.7630\n","Epoch: 017, Train: 0.9667, Val Acc: 0.7660, Test Acc: 0.7600\n","Epoch: 018, Train: 0.9667, Val Acc: 0.7740, Test Acc: 0.7650\n","Epoch: 019, Train: 0.9667, Val Acc: 0.7800, Test Acc: 0.7610\n","Epoch: 020, Train: 0.9667, Val Acc: 0.7820, Test Acc: 0.7680\n","Epoch: 021, Train: 0.9667, Val Acc: 0.7860, Test Acc: 0.7790\n","Epoch: 022, Train: 0.9667, Val Acc: 0.7880, Test Acc: 0.7820\n","Epoch: 023, Train: 0.9667, Val Acc: 0.7920, Test Acc: 0.7880\n","Epoch: 024, Train: 0.9667, Val Acc: 0.7940, Test Acc: 0.7880\n","Epoch: 025, Train: 0.9833, Val Acc: 0.7920, Test Acc: 0.7780\n","Epoch: 026, Train: 0.9833, Val Acc: 0.8000, Test Acc: 0.7750\n","Epoch: 027, Train: 0.9667, Val Acc: 0.8080, Test Acc: 0.7800\n","Epoch: 028, Train: 0.9667, Val Acc: 0.8040, Test Acc: 0.7920\n","Epoch: 029, Train: 0.9667, Val Acc: 0.7980, Test Acc: 0.7880\n","Epoch: 030, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7880\n","Epoch: 031, Train: 0.9833, Val Acc: 0.8040, Test Acc: 0.7800\n","Epoch: 032, Train: 0.9833, Val Acc: 0.8020, Test Acc: 0.7770\n","Epoch: 033, Train: 0.9667, Val Acc: 0.8120, Test Acc: 0.7900\n","Epoch: 034, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7900\n","Epoch: 035, Train: 0.9833, Val Acc: 0.7960, Test Acc: 0.7930\n","Epoch: 036, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7900\n","Epoch: 037, Train: 0.9833, Val Acc: 0.7960, Test Acc: 0.7930\n","Epoch: 038, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7920\n","Epoch: 039, Train: 0.9833, Val Acc: 0.8040, Test Acc: 0.7910\n","Epoch: 040, Train: 0.9833, Val Acc: 0.7980, Test Acc: 0.7930\n","Epoch: 041, Train: 0.9833, Val Acc: 0.7960, Test Acc: 0.7940\n","Epoch: 042, Train: 0.9833, Val Acc: 0.7960, Test Acc: 0.7900\n","Epoch: 043, Train: 0.9833, Val Acc: 0.7940, Test Acc: 0.7920\n","Epoch: 044, Train: 0.9833, Val Acc: 0.8000, Test Acc: 0.7950\n","Epoch: 045, Train: 0.9833, Val Acc: 0.7960, Test Acc: 0.7950\n","Epoch: 046, Train: 0.9833, Val Acc: 0.8000, Test Acc: 0.8000\n","Epoch: 047, Train: 0.9833, Val Acc: 0.8020, Test Acc: 0.7820\n","Epoch: 048, Train: 0.9833, Val Acc: 0.8000, Test Acc: 0.7730\n","Epoch: 049, Train: 0.9833, Val Acc: 0.8060, Test Acc: 0.8020\n","Epoch: 050, Train: 0.9833, Val Acc: 0.8020, Test Acc: 0.7940\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z6QACYSRbFqx"},"execution_count":null,"outputs":[]}]}